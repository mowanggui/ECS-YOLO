# YOLOv3 ðŸš€ by Ultralytics, GPL-3.0 license
"""
YOLO-specific modules

Usage:
    $ python path/to/models/yolo.py --cfg yolov3.yaml
"""

'''===============================================ä¸€ã€å¯¼å…¥åŒ…==================================================='''
'''======================1.å¯¼å…¥å®‰è£…å¥½çš„pythonåº“====================='''
import argparse
import sys
from copy import deepcopy
from pathlib import Path

'''===================2.èŽ·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„========================'''
FILE = Path(__file__).resolve()  # __file__æŒ‡çš„æ˜¯å½“å‰æ–‡ä»¶(å³val.py),FILEæœ€ç»ˆä¿å­˜ç€å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„,æ¯”å¦‚D://yolov5/modles/yolo.py
ROOT = FILE.parents[1]  # root directory ä¿å­˜ç€å½“å‰é¡¹ç›®çš„çˆ¶ç›®å½•,æ¯”å¦‚ D://yolov5
if str(ROOT) not in sys.path:  # sys.pathå³å½“å‰pythonçŽ¯å¢ƒå¯ä»¥è¿è¡Œçš„è·¯å¾„,å‡å¦‚å½“å‰é¡¹ç›®ä¸åœ¨è¯¥è·¯å¾„ä¸­,å°±æ— æ³•è¿è¡Œå…¶ä¸­çš„æ¨¡å—,æ‰€ä»¥å°±éœ€è¦åŠ è½½è·¯å¾„
    sys.path.append(str(ROOT))  # add ROOT to PATH æŠŠROOTæ·»åŠ åˆ°è¿è¡Œè·¯å¾„ä¸Š
# ROOT = ROOT.relative_to(Path.cwd())  # relative ROOTè®¾ç½®ä¸ºç›¸å¯¹è·¯å¾„
# ä½¿ç”¨ Path.cwd() æ–¹æ³•å¯ä»¥èŽ·å–å½“å‰å·¥ä½œç›®å½•ã€‚

'''===================3..åŠ è½½è‡ªå®šä¹‰æ¨¡å—============================'''
from models.common import *  # yoloçš„ç½‘ç»œç»“æž„(yolo)
from models.common2 import AIFI, RepC3

import sys
sys.path.append('/home/algointern/project/EMS-YOLO-main/utils')

from models.experimental import *  # å¯¼å…¥åœ¨çº¿ä¸‹è½½æ¨¡å—
from autoanchor import check_anchor_order  # å¯¼å…¥æ£€æŸ¥anchorsåˆæ³•æ€§çš„å‡½æ•°
from general import LOGGER, check_version, check_yaml, make_divisible, print_args  # å®šä¹‰äº†ä¸€äº›å¸¸ç”¨çš„å·¥å…·å‡½æ•°
from plots import feature_visualization  # å®šä¹‰äº†Annotatorç±»ï¼Œå¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶çŸ©å½¢æ¡†å’Œæ ‡æ³¨ä¿¡æ¯
from torch_utils import (copy_attr, fuse_conv_and_bn, initialize_weights, model_info, scale_img, select_device,
                               time_sync)  # å®šä¹‰äº†ä¸€äº›ä¸ŽPyTorchæœ‰å…³çš„å·¥å…·å‡½æ•°

#from spikingjelly.activation_based import neuron, functional, surrogate, layer

# å¯¼å…¥thopåŒ… ç”¨äºŽè®¡ç®—FLOPs
try:
    import thop  # for FLOPs computation ç”¨äºŽFLOPsè®¡ç®—
except ImportError:
    thop = None
time_window = 8

'''===============================================äºŒã€Detectæ¨¡å—==================================================='''
'''
   Detectæ¨¡å—æ˜¯ç”¨æ¥æž„å»ºDetectå±‚çš„ï¼Œå°†è¾“å…¥feature map é€šè¿‡ä¸€ä¸ªå·ç§¯æ“ä½œå’Œå…¬å¼è®¡ç®—åˆ°æˆ‘ä»¬æƒ³è¦çš„shape, ä¸ºåŽé¢çš„è®¡ç®—æŸå¤±æˆ–è€…NMSåŽå¤„ç†ä½œå‡†å¤‡
'''


class Detect(nn.Module):
    stride = None  # strides computed during build åœ¨æž„å»ºè¿‡ç¨‹ä¸­è®¡ç®—æ­¥é•¿ï¼ˆç‰¹å¾å›¾çš„ç¼©æ”¾æ­¥é•¿ï¼‰
    onnx_dynamic = False  # ONNX export parameter ONNXå¯¼å‡ºå‚æ•°ï¼ˆONNXåŠ¨æ€é‡åŒ–ï¼‰

    '''===================1.èŽ·å–é¢„æµ‹å¾—åˆ°çš„å‚æ•°============================'''

    def __init__(self, nc=80, anchors=(), ch=(), inplace=True, use_cupy=False):  # detection layer
        super().__init__()
        # nc: æ•°æ®é›†ç±»åˆ«æ•°é‡
        self.nc = nc  # number of classes
        # no: è¡¨ç¤ºæ¯ä¸ªanchorçš„è¾“å‡ºæ•°ï¼Œå‰ncä¸ª01å­—ç¬¦å¯¹åº”ç±»åˆ«ï¼ŒåŽ5ä¸ªå¯¹åº”ï¼šæ˜¯å¦æœ‰ç›®æ ‡ï¼Œç›®æ ‡æ¡†çš„ä¸­å¿ƒï¼Œç›®æ ‡æ¡†çš„å®½é«˜
        self.no = nc + 5  # number of outputs per anchor
        # nl: è¡¨ç¤ºé¢„æµ‹å±‚æ•°ï¼Œyolov5æ˜¯3å±‚é¢„æµ‹
        self.nl = len(anchors)  # number of detection layers
        # na: è¡¨ç¤ºanchorsçš„æ•°é‡ï¼Œé™¤ä»¥2æ˜¯å› ä¸º[10,13, 16,30, 33,23]è¿™ä¸ªé•¿åº¦æ˜¯6ï¼Œå¯¹åº”3ä¸ªanchor
        self.na = len(anchors[0]) // 2  # number of anchors
        # grid: è¡¨ç¤ºåˆå§‹åŒ–gridåˆ—è¡¨å¤§å°ï¼Œä¸‹é¢ä¼šè®¡ç®—gridï¼Œgridå°±æ˜¯æ¯ä¸ªæ ¼å­çš„xï¼Œyåæ ‡ï¼ˆæ•´æ•°ï¼Œæ¯”å¦‚0-19ï¼‰ï¼Œå·¦ä¸Šè§’ä¸º(1,1),å³ä¸‹è§’ä¸º(input.w/stride,input.h/stride)
        self.grid = [torch.zeros(1)] * self.nl  # init grid
        # anchor_grid: è¡¨ç¤ºåˆå§‹åŒ–anchor_gridåˆ—è¡¨å¤§å°ï¼Œç©ºåˆ—è¡¨
        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid
        # æ³¨å†Œå¸¸é‡anchorï¼Œå¹¶å°†é¢„é€‰æ¡†ï¼ˆå°ºå¯¸ï¼‰ä»¥æ•°å¯¹å½¢å¼å­˜å…¥ï¼Œå¹¶å‘½åä¸ºanchors
        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)
        # æ¯ä¸€å¼ è¿›è¡Œä¸‰æ¬¡é¢„æµ‹ï¼Œæ¯ä¸€ä¸ªé¢„æµ‹ç»“æžœåŒ…å«nc+5ä¸ªå€¼
        # (n, 255, 80, 80),(n, 255, 40, 40),(n, 255, 20, 20) --> ch=(255, 255, 255)
        # 255 -> (nc+5)*3 ===> ä¸ºäº†æå–å‡ºé¢„æµ‹æ¡†çš„ä½ç½®ä¿¡æ¯ä»¥åŠé¢„æµ‹æ¡†å°ºå¯¸ä¿¡æ¯
        self.m = nn.ModuleList(Snn_Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv
        # inplace: ä¸€èˆ¬éƒ½æ˜¯Trueï¼Œé»˜è®¤ä¸ä½¿ç”¨AWSï¼ŒInferentiaåŠ é€Ÿ
        self.inplace = inplace  # use in-place ops (e.g. slice assignment)

    # å¦‚æžœæ¨¡åž‹ä¸è®­ç»ƒé‚£ä¹ˆå°†ä¼šå¯¹è¿™äº›é¢„æµ‹å¾—åˆ°çš„å‚æ•°è¿›ä¸€æ­¥å¤„ç†,ç„¶åŽè¾“å‡º,å¯ä»¥æ–¹ä¾¿åŽæœŸçš„ç›´æŽ¥è°ƒç”¨
    # åŒ…å«äº†ä¸‰ä¸ªä¿¡æ¯pred_box [x,y,w,h] pred_conf[confidence] pre_cls[cls0,cls1,cls2,...clsn]

    '''===================2.å‘å‰ä¼ æ’­============================'''

    def forward(self, x):

        z = []  # inference output æŽ¨ç†è¾“å‡º
        for i in range(self.nl):
            x[i] = self.m[i](x[i])  # conv
            times, bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            # ç»´åº¦é‡æŽ’åˆ—: bs, å…ˆéªŒæ¡†ç»„æ•°, æ£€æµ‹æ¡†è¡Œæ•°, æ£€æµ‹æ¡†åˆ—æ•°, å±žæ€§æ•° + åˆ†ç±»æ•°
            x[i] = x[i].view(times, bs, self.na, self.no, ny, nx).permute(0, 1, 2, 4, 5, 3).contiguous()

            x[i] = x[i].sum(dim=0) / x[i].size()[0]
            '''
            å‘å‰ä¼ æ’­æ—¶éœ€è¦å°†ç›¸å¯¹åæ ‡è½¬æ¢åˆ°gridç»å¯¹åæ ‡ç³»ä¸­
            '''
            if not self.training:  # inference
                '''
                ç”Ÿæˆåæ ‡ç³»
                grid[i].shape = [1,1,ny,nx,2]
                                [[[[1,1],[1,2],...[1,nx]],
                                [[2,1],[2,2],...[2,nx]],
                                ...,
                                [[ny,1],[ny,2],...[ny,nx]]]]
                '''
                # æ¢è¾“å…¥åŽé‡æ–°è®¾å®šé”šæ¡†
                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
                    # åŠ è½½ç½‘æ ¼ç‚¹åæ ‡ å…ˆéªŒæ¡†å°ºå¯¸
                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)

                '''
                æŒ‰æŸå¤±å‡½æ•°çš„å›žå½’æ–¹å¼æ¥è½¬æ¢åæ ‡
                '''
                y = x[i].sigmoid()
                # æ”¹å˜åŽŸæ•°æ® è®¡ç®—å®šä½å‚æ•°
                if self.inplace:  # æ‰§è¡Œè¿™é‡Œ
                    # grid: ä½ç½®åŸºå‡† æˆ–è€…ç†è§£ä¸º cellçš„é¢„æµ‹åˆå§‹ä½ç½®ï¼Œè€Œy[..., 0:2]æ˜¯ä½œä¸ºåœ¨gridåæ ‡åŸºç¡€ä¸Šçš„ä½ç½®åç§»
                    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    # anchor_grid: é¢„æµ‹æ¡†åŸºå‡† æˆ–è€…ç†è§£ä¸º é¢„æµ‹æ¡†çš„åˆå§‹ä½ç½®ï¼Œè€Œ y[..., 2:4]æ˜¯ä½œä¸ºé¢„æµ‹æ¡†ä½ç½®çš„è°ƒ
                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                else:  # for  on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
                    # stride: æ˜¯ä¸€ä¸ªgrid cellçš„å®žé™…å°ºå¯¸
                    # ç»è¿‡sigmoid, å€¼èŒƒå›´å˜æˆäº†(0-1),ä¸‹ä¸€è¡Œä»£ç å°†å€¼å˜æˆèŒƒå›´ï¼ˆ-0.5ï¼Œ1.5ï¼‰
                    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    # èŒƒå›´å˜æˆ(0-4)å€ï¼Œè®¾ç½®ä¸º4å€çš„åŽŸå› æ˜¯ä¸‹å±‚çš„æ„Ÿå—é‡Žæ˜¯ä¸Šå±‚çš„2å€
                    # å› ä¸‹å±‚æ³¨é‡æ£€æµ‹å¤§ç›®æ ‡ï¼Œç›¸å¯¹æ¯”ä¸Šå±‚è€Œè¨€ï¼Œè®¡ç®—é‡æ›´å°ï¼Œ4å€æ˜¯ä¸€ä¸ªæŠ˜ä¸­çš„é€‰æ‹©
                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                    y = torch.cat((xy, wh, y[..., 4:]), -1)
                # å­˜å‚¨æ¯ä¸ªç‰¹å¾å›¾æ£€æµ‹æ¡†çš„ä¿¡æ¯
                z.append(y.view(bs, -1, self.no))

        # è®­ç»ƒé˜¶æ®µç›´æŽ¥è¿”å›žx
        # é¢„æµ‹é˜¶æ®µè¿”å›ž3ä¸ªç‰¹å¾å›¾æ‹¼æŽ¥çš„ç»“æžœ
        return x if self.training else (torch.cat(z, 1), x)

    '''===================3.ç›¸å¯¹åæ ‡è½¬æ¢åˆ°gridç»å¯¹åæ ‡ç³»============================'''

    def _make_grid(self, nx=20, ny=20, i=0):
        d = self.anchors[i].device
        if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility
            yv, xv = torch.meshgrid([torch.arange(ny).to(d), torch.arange(nx).to(d)], indexing='ij')
        else:
            yv, xv = torch.meshgrid([torch.arange(ny).to(d), torch.arange(nx).to(d)])
        # grid --> (20, 20, 2), å¤åˆ¶æˆ3å€ï¼Œå› ä¸ºæ˜¯ä¸‰ä¸ªæ¡† -> (3, 20, 20, 2)
        grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()
        # anchor_gridå³æ¯ä¸ªæ ¼å­å¯¹åº”çš„anchorå®½é«˜ï¼Œstrideæ˜¯ä¸‹é‡‡æ ·çŽ‡ï¼Œä¸‰å±‚åˆ†åˆ«æ˜¯8ï¼Œ16ï¼Œ32
        anchor_grid = (self.anchors[i].clone() * self.stride[i]) \
            .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()
        return grid, anchor_grid


'''===============================================ä¸‰ã€Modelæ¨¡å—==================================================='''


class Model(nn.Module):
    '''===================1.__init__å‡½æ•°==========================='''

    def __init__(self, cfg='yolov3.yaml', ch=3, nc=None, anchors=None,
                 use_cupy=False):  # model, input channels, number of classes
        """
            :params cfg:YOLO v5æ¨¡åž‹é…ç½®æ–‡ä»¶ è¿™é‡Œä½¿ç”¨yolov5sæ¨¡åž‹
            :params ch: è¾“å…¥å›¾ç‰‡çš„é€šé“æ•° é»˜è®¤ä¸º3
            :params nc: æ•°æ®é›†çš„ç±»åˆ«ä¸ªæ•°
            :anchors: è¡¨ç¤ºanchoræ¡†, ä¸€èˆ¬æ˜¯None
        """
        # çˆ¶ç±»çš„æž„é€ æ–¹æ³•
        super().__init__()
        # æ£€æŸ¥ä¼ å…¥çš„å‚æ•°æ ¼å¼ï¼Œå¦‚æžœcfgæ˜¯åŠ è½½å¥½çš„å­—å…¸ç»“æžœ
        if isinstance(cfg, dict):
            # ç›´æŽ¥ä¿å­˜åˆ°æ¨¡åž‹ä¸­
            self.yaml = cfg  # model dict æ¨¡åž‹dictç±»åž‹
        # è‹¥ä¸æ˜¯å­—å…¸ åˆ™ä¸ºyamlæ–‡ä»¶è·¯å¾„
        else:  # is *.yamlæ‰§è¡Œè¿™é‡Œ
            # å¯¼å…¥yamlæ–‡ä»¶
            import yaml  # for torch hub
            # ä¿å­˜æ–‡ä»¶åï¼šcfg file name = yolov5s.yaml
            self.yaml_file = Path(cfg).name
            # å¦‚æžœé…ç½®æ–‡ä»¶ä¸­æœ‰ä¸­æ–‡ï¼Œæ‰“å¼€æ—¶è¦åŠ encodingå‚æ•°
            with open(cfg, encoding='ascii', errors='ignore') as f:
                # å°†yamlæ–‡ä»¶åŠ è½½ä¸ºå­—å…¸
                self.yaml = yaml.safe_load(f)  # model dict å–åˆ°é…ç½®æ–‡ä»¶ä¸­æ¯æ¡çš„ä¿¡æ¯ï¼ˆæ²¡æœ‰æ³¨é‡Šå†…å®¹ï¼‰

        '''===================2.èŽ·å–è¾“å…¥é€šé“============================'''
        # Define model
        # æ­å»ºæ¨¡åž‹
        # yaml.get('ch', ch)è¡¨ç¤ºè‹¥ä¸å­˜åœ¨é”®'ch',åˆ™è¿”å›žå€¼ch
        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels
        # åˆ¤æ–­ç±»çš„é€šé“æ•°å’Œyamlä¸­çš„é€šé“æ•°æ˜¯å¦ç›¸ç­‰ï¼Œä¸€èˆ¬ä¸æ‰§è¡Œï¼Œå› ä¸ºnc=self.yaml['nc']æ’æˆç«‹
        if nc and nc != self.yaml['nc']:
            # åœ¨ç»ˆç«¯ç»™å‡ºæç¤º
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
            # å°†yamlä¸­çš„å€¼ä¿®æ”¹ä¸ºæž„é€ æ–¹æ³•ä¸­çš„å€¼
            self.yaml['nc'] = nc  # override yaml value è¦†ç›–yamlå€¼
        # é‡å†™anchorï¼Œä¸€èˆ¬ä¸æ‰§è¡Œ, å› ä¸ºä¼ è¿›æ¥çš„anchorsä¸€èˆ¬éƒ½æ˜¯None
        if anchors:
            LOGGER.info(f'Overriding model.yaml anchors with anchors={anchors}')
            self.yaml['anchors'] = round(anchors)  # override yaml value
        # è§£æžæ¨¡åž‹ï¼Œself.modelæ˜¯è§£æžåŽçš„æ¨¡åž‹ self.saveæ˜¯æ¯ä¸€å±‚ä¸Žä¹‹ç›¸è¿žçš„å±‚
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch], use_cupy=use_cupy)  # model, savelist
        # åŠ è½½æ¯ä¸€ç±»çš„ç±»åˆ«å
        self.names = [str(i) for i in range(self.yaml['nc'])]  # default names
        # inplaceæŒ‡çš„æ˜¯åŽŸåœ°æ“ä½œ å¦‚x+=1 æœ‰åˆ©äºŽèŠ‚çº¦å†…å­˜
        # self.inplace=True  é»˜è®¤True  ä¸ä½¿ç”¨åŠ é€ŸæŽ¨ç†
        self.inplace = self.yaml.get('inplace', True)

        '''===================3.èŽ·å–Detectè¾“å‡ºæ¨¡å—============================'''
        # Build strides, anchors
        # æž„é€ æ­¥é•¿ã€å…ˆéªŒæ¡†
        m = self.model[-1]  # Detect()
        # åˆ¤æ–­æœ€åŽä¸€å±‚æ˜¯å¦ä¸ºDetectå±‚
        if isinstance(m, Detect):
            # å®šä¹‰ä¸€ä¸ª256 * 256å¤§å°çš„è¾“å…¥
            s = 256  # 2x min stride
            m.inplace = self.inplace
            # ä¿å­˜ç‰¹å¾å±‚çš„stride,å¹¶ä¸”å°†anchorå¤„ç†æˆç›¸å¯¹äºŽç‰¹å¾å±‚çš„æ ¼å¼
            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward
            # åŽŸå§‹å®šä¹‰çš„anchoræ˜¯åŽŸå§‹å›¾ç‰‡ä¸Šçš„åƒç´ å€¼ï¼Œè¦å°†å…¶ç¼©æ”¾è‡³ç‰¹å¾å›¾çš„å¤§å°
            m.anchors /= m.stride.view(-1, 1, 1)
            # æ£€æŸ¥anchoré¡ºåºä¸Žstrideé¡ºåºæ˜¯å¦ä¸€è‡´# æ£€æŸ¥anchoré¡ºåºä¸Žstrideé¡ºåºæ˜¯å¦ä¸€è‡´
            check_anchor_order(m)
            # å°†æ­¥é•¿ä¿å­˜è‡³æ¨¡åž‹
            self.stride = m.stride
            # å°†æ­¥é•¿ä¿å­˜è‡³æ¨¡åž‹
            self._initialize_biases()  # only run once

        # Init weights, biases åˆå§‹åŒ–æƒé‡ï¼Œåå·®
        initialize_weights(self)
        # æ‰“å°æ¨¡åž‹ä¿¡æ¯
        self.info()
        LOGGER.info('')

    '''===================4.æ•°æ®å¢žå¼º============================'''

    # ===1.forward():ç®¡ç†å‰å‘ä¼ æ’­å‡½æ•°=== #
    def forward(self, x, augment=False, profile=False, visualize=False):
        input = torch.zeros(time_window, x.size()[0], x.size()[1], x.size()[2], x.size()[3], device=x.device,
                            dtype=x.dtype)
        for i in range(time_window):
            input[i] = x

        # æ˜¯å¦åœ¨æµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨æ•°æ®å¢žå¼º
        if augment:
            # å¢žå¼ºè®­ç»ƒï¼Œå¯¹æ•°æ®é‡‡å–äº†ä¸€äº›äº†æ“ä½œ
            return self._forward_augment(x)  # augmented inference, None å¢žå¼ºæŽ¨ç†ï¼Œæ— 
        # é»˜è®¤æ‰§è¡Œï¼Œæ­£å¸¸å‰å‘æŽ¨ç†
        return self._forward_once(input, profile, visualize)  # single-scale inference, train å•å°ºåº¦æŽ¨ç†ï¼Œè®­ç»ƒ

    # ===2._forward_augment():æŽ¨ç†çš„forward=== #
    # å°†å›¾ç‰‡è¿›è¡Œè£å‰ª,å¹¶åˆ†åˆ«é€å…¥æ¨¡åž‹è¿›è¡Œæ£€æµ‹
    def _forward_augment(self, x):
        # èŽ·å¾—å›¾åƒçš„é«˜å’Œå®½
        img_size = x.shape[-2:]  # height, width
        # sæ˜¯è§„æ¨¡
        s = [1, 0.83, 0.67]  # scales å°ºåº¦
        # flipæ˜¯ç¿»è½¬ï¼Œè¿™é‡Œçš„å‚æ•°è¡¨ç¤ºæ²¿ç€å“ªä¸ªè½´ç¿»è½¬
        f = [None, 3, None]  # flips (2-ud, 3-lr)
        y = []  # outputs
        for si, fi in zip(s, f):
            # scale_imgå‡½æ•°çš„ä½œç”¨å°±æ˜¯æ ¹æ®ä¼ å…¥çš„å‚æ•°ç¼©æ”¾å’Œç¿»è½¬å›¾åƒ
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))
            # æ¨¡åž‹å‰å‘ä¼ æ’­
            yi = self._forward_once(xi)[0]  # forward
            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save
            #  æ¢å¤æ•°æ®å¢žå¼ºå‰çš„æ¨¡æ ·
            yi = self._descale_pred(yi, fi, si, img_size)
            y.append(yi)
        # å¯¹ä¸åŒå°ºå¯¸è¿›è¡Œä¸åŒç¨‹åº¦çš„ç­›é€‰
        y = self._clip_augmented(y)  # clip augmented tails
        return torch.cat(y, 1), None  # augmented inference, train

    # ===3._forward_once():è®­ç»ƒçš„forward=== #
    def _forward_once(self, x, profile=False, visualize=False):  # æ‰§è¡Œè¿™é‡Œã€1,256,16,16ã€‘
        # å„ç½‘ç»œå±‚è¾“å‡º, å„ç½‘ç»œå±‚æŽ¨å¯¼è€—æ—¶
        # y: å­˜æ”¾ç€self.save=Trueçš„æ¯ä¸€å±‚çš„è¾“å‡ºï¼Œå› ä¸ºåŽé¢çš„å±‚ç»“æž„concatç­‰æ“ä½œè¦ç”¨åˆ°
        # dt: åœ¨profileä¸­åšæ€§èƒ½è¯„ä¼°æ—¶ä½¿ç”¨
        y, dt = [], []  # outputs
        # éåŽ†modelçš„å„ä¸ªæ¨¡å—
        for m in self.model:
            # m.f å°±æ˜¯è¯¥å±‚çš„è¾“å…¥æ¥æºï¼Œå¦‚æžœä¸ä¸º-1é‚£å°±ä¸æ˜¯ä»Žä¸Šä¸€å±‚è€Œæ¥
            if m.f != -1:  # if not from previous layer
                # from å‚æ•°æŒ‡å‘çš„ç½‘ç»œå±‚è¾“å‡ºçš„åˆ—è¡¨
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            # æµ‹è¯•è¯¥ç½‘ç»œå±‚çš„æ€§èƒ½
            if profile:
                self._profile_one_layer(m, x, dt)
            # ä½¿ç”¨è¯¥ç½‘ç»œå±‚è¿›è¡ŒæŽ¨å¯¼, å¾—åˆ°è¯¥ç½‘ç»œå±‚çš„è¾“å‡º
            x = m(x)  # run
            # å­˜æ”¾ç€self.saveçš„æ¯ä¸€å±‚çš„è¾“å‡ºï¼Œå› ä¸ºåŽé¢éœ€è¦ç”¨æ¥ä½œconcatç­‰æ“ä½œè¦ç”¨åˆ°  ä¸åœ¨self.saveå±‚çš„è¾“å‡ºå°±ä¸ºNone
            y.append(x if m.i in self.save else None)  # save output m.iæ˜¯ç¬¬å‡ å±‚ï¼Œself
            # å°†æ¯ä¸€å±‚çš„è¾“å‡ºç»“æžœä¿å­˜åˆ°y
            if visualize:
                # ç»˜åˆ¶è¯¥ batch ä¸­ç¬¬ä¸€å¼ å›¾åƒçš„ç‰¹å¾å›¾
                feature_visualization(x, m.type, m.i, save_dir=visualize)

        # functional.reset_net(self.model)
        # print('=======')
        # print(x[0].shape)#torch.Size([32, 3, 40, 40, 85])
        # # #torch.Size([32, 3, 20, 20, 85])
        # print(x[1].shape)
        return x

    # ===4._descale_pred():å°†æŽ¨ç†ç»“æžœæ¢å¤åˆ°åŽŸå›¾å°ºå¯¸(é€†æ“ä½œ)=== #
    def _descale_pred(self, p, flips, scale, img_size):
        # de-scale predictions following augmented inference (inverse operation)
        if self.inplace:
            # æŠŠx,y,w,hæ¢å¤æˆåŽŸæ¥çš„å¤§å°
            p[..., :4] /= scale  # de-scale
            # bs c h w  å½“flips=2æ˜¯å¯¹hè¿›è¡Œå˜æ¢ï¼Œé‚£å°±æ˜¯ä¸Šä¸‹è¿›è¡Œç¿»è½¬
            if flips == 2:
                p[..., 1] = img_size[0] - p[..., 1]  # de-flip ud
            # åŒç†flips=3æ˜¯å¯¹æ°´å¹³è¿›è¡Œç¿»è½¬
            elif flips == 3:
                p[..., 0] = img_size[1] - p[..., 0]  # de-flip lr
        else:
            x, y, wh = p[..., 0:1] / scale, p[..., 1:2] / scale, p[..., 2:4] / scale  # de-scale
            if flips == 2:
                y = img_size[0] - y  # de-flip ud
            elif flips == 3:
                x = img_size[1] - x  # de-flip lr
            p = torch.cat((x, y, wh, p[..., 4:]), -1)
        return p

    # ===5._clip_augmentedï¼ˆï¼‰:TTAçš„æ—¶å€™å¯¹åŽŸå›¾ç‰‡è¿›è¡Œè£å‰ª=== #
    # ä¹Ÿæ˜¯ä¸€ç§æ•°æ®å¢žå¼ºæ–¹å¼ï¼Œç”¨åœ¨TTAæµ‹è¯•çš„æ—¶å€™
    def _clip_augmented(self, y):
        # Clip  augmented inference tails
        nl = self.model[-1].nl  # number of detection layers (P3-P5) æ£€æµ‹å±‚æ•°(P3-P5)
        g = sum(4 ** x for x in range(nl))  # grid points
        e = 1  # exclude layer count æŽ’é™¤å±‚æ•°
        i = (y[0].shape[1] // g) * sum(4 ** x for x in range(e))  # indices
        y[0] = y[0][:, :-i]  # large
        i = (y[-1].shape[1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # indices
        y[-1] = y[-1][:, i:]  # small
        return y

    # ===6._profile_one_layerï¼ˆï¼‰:æ‰“å°æ—¥å¿—ä¿¡æ¯=== #
    def _profile_one_layer(self, m, x, dt):
        c = isinstance(m, Detect)  # is final layer, copy input as inplace fix
        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPs
        t = time_sync()
        for _ in range(10):
            m(x.copy() if c else x)
        dt.append((time_sync() - t) * 100)
        if m == self.model[0]:
            LOGGER.info(f"{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  {'module'}")
        LOGGER.info(f'{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}')
        if c:
            LOGGER.info(f"{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total")

    # ===7._initialize_biasesï¼ˆï¼‰:åˆå§‹åŒ–åç½®biasesä¿¡æ¯=== #
    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency
        # https://arxiv.org/abs/1708.02002 section 3.3
        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.
        m = self.model[-1]  # Detect() module
        for mi, s in zip(m.m, m.stride):  # from
            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)
            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)
            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # cls
            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)

    # ===8._print_biasesï¼ˆï¼‰:æ‰“å°åç½®biasesä¿¡æ¯=== #
    def _print_biases(self):
        """
                æ‰“å°æ¨¡åž‹ä¸­æœ€åŽDetectå±‚çš„åç½®biasä¿¡æ¯(ä¹Ÿå¯ä»¥ä»»é€‰å“ªäº›å±‚biasä¿¡æ¯)
        """
        m = self.model[-1]  # Detect() module
        for mi in m.m:  # from
            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)
            LOGGER.info(
                ('%6g Conv2d.bias:' + '%10.3g' * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))

    # def _print_weights(self):
    #     for m in self.model.modules():
    #         if type(m) is Bottleneck:
    #             LOGGER.info('%10.3g' % (m.w.detach().sigmoid() * 2))  # shortcut weights

    # ===9.fuseï¼ˆï¼‰:å°†Conv2d+BNè¿›è¡Œèžåˆ=== #
    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
        LOGGER.info('Fusing layers... ')
        for m in self.model.modules():
            # å¦‚æžœå½“å‰å±‚æ˜¯å·ç§¯å±‚Convä¸”æœ‰bnç»“æž„, é‚£ä¹ˆå°±è°ƒç”¨fuse_conv_and_bnå‡½æ•°è®²convå’Œbnè¿›è¡Œèžåˆ, åŠ é€ŸæŽ¨ç†
            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):
                # æ›´æ–°å·ç§¯å±‚
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                # ç§»é™¤bn
                delattr(m, 'bn')  # remove batchnorm
                # æ›´æ–°å‰å‘ä¼ æ’­
                m.forward = m.forward_fuse  # update forward
        # æ‰“å°conv+bnèžåˆåŽçš„æ¨¡åž‹ä¿¡æ¯
        self.info()
        return self

    # ===10.autoshapeï¼ˆï¼‰:æ‰©å±•æ¨¡åž‹åŠŸèƒ½=== #
    def autoshape(self):  # add AutoShape module
        LOGGER.info('Adding AutoShape... ')
        #  æ­¤æ—¶æ¨¡åž‹åŒ…å«å‰å¤„ç†ã€æŽ¨ç†ã€åŽå¤„ç†çš„æ¨¡å—(é¢„å¤„ç† + æŽ¨ç† + nms)
        m = AutoShape(self)  # wrap model
        copy_attr(m, self, include=('yaml', 'nc', 'hyp', 'names', 'stride'), exclude=())  # copy attributes
        return m

    # ===11.info():æ‰“å°æ¨¡åž‹ç»“æž„ä¿¡æ¯=== #
    def info(self, verbose=False, img_size=640):  # print model information
        model_info(self, verbose, img_size)

    # ===12._apply():å°†æ¨¡å—è½¬ç§»åˆ° CPU/ GPUä¸Š=== #
    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)
        m = self.model[-1]  # Detect()
        if isinstance(m, Detect):
            m.stride = fn(m.stride)
            m.grid = list(map(fn, m.grid))
            if isinstance(m.anchor_grid, list):
                m.anchor_grid = list(map(fn, m.anchor_grid))
        return self


'''===============================================å››ã€parse_modelæ¨¡å—==================================================='''


def parse_model(d, ch, use_cupy):  # model_dict, input_channels(3)

    '''===================1. èŽ·å–å¯¹åº”å‚æ•°============================'''

    # ä½¿ç”¨ logging æ¨¡å—è¾“å‡ºåˆ—æ ‡ç­¾
    LOGGER.info(f"\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}")
    # èŽ·å–anchorsï¼Œncï¼Œdepth_multipleï¼Œwidth_multipleï¼Œè¿™äº›å‚æ•°åœ¨ä»‹ç»yamlæ—¶å·²ç»ä»‹ç»è¿‡
    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']
    # na: æ¯ç»„å…ˆéªŒæ¡†åŒ…å«çš„å…ˆéªŒæ¡†æ•°
    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors
    # no: na * å±žæ€§æ•° (5 + åˆ†ç±»æ•°)
    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)

    '''===================2. å¼€å§‹æ­å»ºç½‘ç»œ============================'''

    # ç½‘ç»œå•å…ƒåˆ—è¡¨, ç½‘ç»œè¾“å‡ºå¼•ç”¨åˆ—è¡¨, å½“å‰çš„è¾“å‡ºé€šé“æ•°
    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
    # è¯»å– backbone, head ä¸­çš„ç½‘ç»œå•å…ƒ
    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args
        # åˆ©ç”¨ eval å‡½æ•°, è¯»å– model å‚æ•°å¯¹åº”çš„ç±»å
        m = eval(m) if isinstance(m, str) else m  # eval strings
        # ä½¿ç”¨ eval å‡½æ•°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå˜é‡
        for j, a in enumerate(args):
            try:
                args[j] = eval(a) if isinstance(a, str) else a  # eval strings
            except NameError:
                pass

        '''===================3. æ›´æ–°å½“å‰å±‚çš„å‚æ•°ï¼Œè®¡ç®—c2============================'''
        # depth gain: æŽ§åˆ¶æ·±åº¦ï¼Œå¦‚yolov5s: n*0.33ï¼Œn: å½“å‰æ¨¡å—çš„æ¬¡æ•°(é—´æŽ¥æŽ§åˆ¶æ·±åº¦)
        n = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain
        # å½“è¯¥ç½‘ç»œå•å…ƒçš„å‚æ•°å«æœ‰: è¾“å…¥é€šé“æ•°, è¾“å‡ºé€šé“æ•°
        if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,
                 BottleneckCSP, C3, C3TR, C3SPP, C3Ghost, Conv_2, snn_resnet, Concat_res3, Concat_res4, EMA,
                 BasicBlock, BasicBlock_1, BasicBlock_2, BasicBlock_3, Conv_A, CSABlock, LIAFBlock, Conv_LIAF,
                 Bottleneck_2, Conv_3, StarBlock, StarBlock_2, StarBlock_3, StarBlock_4,StarNet,
                 TCSABlock, BasicTCSA, ConcatBlock_ms, BasicBlock_ms, Conv_1, Concat_res2, HAMBlock, ConcatCSA_res2,
                 BasicBlock_ms1, BoT3, BasicBlock_2C3, BasicBlock_1C3, Concat_res2C3, BasicBlock_1C2f, BasicBlock_2C2f,
                 BasicBlock_1n, BasicBlock_1m, BasicBlock_4, Concat_res5, AIFI, RepC3, Silence, ResNetLayerBasic,
                 BasicBlock_5, Concat_res6, MobileNetV3, BasicBlock_6, BasicBlock_1s, Bottleneck_3, StarBlock_5]:
            # c1: å½“å‰å±‚çš„è¾“å…¥channelæ•°; c2: å½“å‰å±‚çš„è¾“å‡ºchannelæ•°(åˆå®š); ch: è®°å½•ç€æ‰€æœ‰å±‚çš„è¾“å‡ºchannelæ•°
            c1, c2 = ch[f], args[0]
            use_cupy = use_cupy
            # no=75ï¼Œåªæœ‰æœ€åŽä¸€å±‚c2=noï¼Œæœ€åŽä¸€å±‚ä¸ç”¨æŽ§åˆ¶å®½åº¦ï¼Œè¾“å‡ºchannelå¿…é¡»æ˜¯no
            if c2 != no:  # if not output
                # width gain: æŽ§åˆ¶å®½åº¦ï¼Œå¦‚yolov5s: c2*0.5; c2: å½“å‰å±‚çš„æœ€ç»ˆè¾“å‡ºchannelæ•°(é—´æŽ¥æŽ§åˆ¶å®½åº¦)
                c2 = make_divisible(c2 * gw, 8)

            '''===================4.ä½¿ç”¨å½“å‰å±‚çš„å‚æ•°æ­å»ºå½“å‰å±‚============================'''
            # åœ¨åˆå§‹argsçš„åŸºç¡€ä¸Šæ›´æ–°ï¼ŒåŠ å…¥å½“å‰å±‚çš„è¾“å…¥channelå¹¶æ›´æ–°å½“å‰å±‚
            # [in_channels, out_channels, *args[1:]]
            args = [c1, c2, *args[1:]]
            # å¦‚æžœå½“å‰å±‚æ˜¯BottleneckCSP / C3 / C3TR / C3Ghost / C3xï¼Œåˆ™éœ€è¦åœ¨argsä¸­åŠ å…¥Bottleneckçš„ä¸ªæ•°
            # [in_channels, out_channels, Bottleneckä¸ªæ•°, Bool(shortcutæœ‰æ— æ ‡è®°)]
            if m in [BottleneckCSP, C3, C3TR, C3Ghost, BoT3, BasicBlock_2C3, BasicBlock_1C3, Concat_res2C3,
                     BasicBlock_1C2f,
                     BasicBlock_2C2f, RepC3]:
                # åœ¨ç¬¬äºŒä¸ªä½ç½®æ’å…¥bottleneckä¸ªæ•°n
                args.insert(2, n)  # number of repeats
                # æ¢å¤é»˜è®¤å€¼1
                n = 1
        elif m is AIFI:
            args = [ch[f], *args]
        # åˆ¤æ–­æ˜¯å¦æ˜¯å½’ä¸€åŒ–æ¨¡å—
        elif m is nn.BatchNorm2d:
            # BNå±‚åªéœ€è¦è¿”å›žä¸Šä¸€å±‚çš„è¾“å‡ºchannel
            args = [ch[f]]
        # åˆ¤æ–­æ˜¯å¦æ˜¯tensorè¿žæŽ¥æ¨¡å—
        elif m is Concat:
            # Concatå±‚åˆ™å°†fä¸­æ‰€æœ‰çš„è¾“å‡ºç´¯åŠ å¾—åˆ°è¿™å±‚çš„è¾“å‡ºchannel
            c2 = sum(ch[x] for x in f)
        # åˆ¤æ–­æ˜¯å¦æ˜¯detectæ¨¡å—
        elif m is Detect:
            # åœ¨argsä¸­åŠ å…¥ä¸‰ä¸ªDetectå±‚çš„è¾“å‡ºchannel
            args.append([ch[x] for x in f])
            if isinstance(args[1], int):  # number of anchors
                args[1] = [list(range(args[1] * 2))] * len(f)
        elif m in {EMA}:
            args = [ch[f], *args]
        elif m in {ContextGuideFusionModule, ContextGuideFusionModulev2}:
            c1 = [ch[x] for x in f]
            c2 = 2 * c1[1]
            args = [c1, *args]
        elif m is Contract:
            c2 = ch[f] * args[0] ** 2
        elif m is Expand:
            c2 = ch[f] // args[0] ** 2
        elif m is ResNetLayerBo:
            c2 = args[1] if args[4] else args[1] * 4
        elif m is HGBlock:
            c1, cm, c2 = ch[f], args[0], args[1]
            args = [c1, cm, c2, *args[2:]]
            if m is HGBlock:
                args.insert(4, n)  # number of repeats
                n = 1
        else:
            c2 = ch[f]

        '''===================5.æ‰“å°å’Œä¿å­˜layersä¿¡æ¯============================'''
        # m_: å¾—åˆ°å½“å‰å±‚çš„moduleï¼Œå°†nä¸ªæ¨¡å—ç»„åˆå­˜æ”¾åˆ°m_é‡Œé¢
        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module

        # æ‰“å°å½“å‰å±‚ç»“æž„çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯
        t = str(m)[8:-2].replace('__main__.', '')  # module type
        # è®¡ç®—è¿™ä¸€å±‚çš„å‚æ•°é‡
        np = sum(x.numel() for x in m_.parameters())  # number params
        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params
        LOGGER.info(f'{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}')  # print

        # æŠŠæ‰€æœ‰å±‚ç»“æž„ä¸­çš„fromä¸æ˜¯-1çš„å€¼è®°ä¸‹ [6,4,14,10,17,20,23]
        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist

        # å°†å½“å‰å±‚ç»“æž„moduleåŠ å…¥layersä¸­
        layers.append(m_)
        if i == 0:
            ch = []  # åŽ»é™¤è¾“å…¥channel[3]
        # æŠŠå½“å‰å±‚çš„è¾“å‡ºchannelæ•°åŠ å…¥ch
        ch.append(c2)
    return nn.Sequential(*layers), sorted(save)


class RTDETRDetectionModel(Model):
    """
    RTDETR (Real-time DEtection and Tracking using Transformers) Detection Model class.

    This class is responsible for constructing the RTDETR architecture, defining loss functions, and facilitating both
    the training and inference processes. RTDETR is an object detection and tracking model that extends from the
    DetectionModel base class.

    Attributes:
        cfg (str): The configuration file path or preset string. Default is 'rtdetr-l.yaml'.
        ch (int): Number of input channels. Default is 3 (RGB).
        nc (int, optional): Number of classes for object detection. Default is None.
        verbose (bool): Specifies if summary statistics are shown during initialization. Default is True.

    Methods:
        init_criterion: Initializes the criterion used for loss calculation.
        loss: Computes and returns the loss during training.
        predict: Performs a forward pass through the network and returns the output.
    """

    def __init__(self, cfg='rtdetr-l.yaml', ch=3, nc=None):
        """
        Initialize the RTDETRDetectionModel.

        Args:
            cfg (str): Configuration file name or path.
            ch (int): Number of input channels.
            nc (int, optional): Number of classes. Defaults to None.
            verbose (bool, optional): Print additional information during initialization. Defaults to True.
        """
        super().__init__(cfg=cfg, ch=ch, nc=nc)

    def init_criterion(self):
        """Initialize the loss criterion for the RTDETRDetectionModel."""
        from ultralytics.models.utils.loss import RTDETRDetectionLoss

        return RTDETRDetectionLoss(nc=self.nc, use_vfl=True)

    def loss(self, batch, preds=None):
        """
        Compute the loss for the given batch of data.

        Args:
            batch (dict): Dictionary containing image and label data.
            preds (torch.Tensor, optional): Precomputed model predictions. Defaults to None.

        Returns:
            (tuple): A tuple containing the total loss and main three losses in a tensor.
        """
        if not hasattr(self, 'criterion'):
            self.criterion = self.init_criterion()

        img = batch['img']
        # NOTE: preprocess gt_bbox and gt_labels to list.
        bs = len(img)
        batch_idx = batch['batch_idx']
        gt_groups = [(batch_idx == i).sum().item() for i in range(bs)]
        targets = {
            'cls': batch['cls'].to(img.device, dtype=torch.long).view(-1),
            'bboxes': batch['bboxes'].to(device=img.device),
            'batch_idx': batch_idx.to(img.device, dtype=torch.long).view(-1),
            'gt_groups': gt_groups}

        preds = self.predict(img, batch=targets) if preds is None else preds
        dec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta = preds if self.training else preds[1]
        if dn_meta is None:
            dn_bboxes, dn_scores = None, None
        else:
            dn_bboxes, dec_bboxes = torch.split(dec_bboxes, dn_meta['dn_num_split'], dim=2)
            dn_scores, dec_scores = torch.split(dec_scores, dn_meta['dn_num_split'], dim=2)

        dec_bboxes = torch.cat([enc_bboxes.unsqueeze(0), dec_bboxes])  # (7, bs, 300, 4)
        dec_scores = torch.cat([enc_scores.unsqueeze(0), dec_scores])

        loss = self.criterion((dec_bboxes, dec_scores),
                              targets,
                              dn_bboxes=dn_bboxes,
                              dn_scores=dn_scores,
                              dn_meta=dn_meta)
        # NOTE: There are like 12 losses in RTDETR, backward with all losses but only show the main three losses.
        return sum(loss.values()), torch.as_tensor([loss[k].detach() for k in ['loss_giou', 'loss_class', 'loss_bbox']],
                                                   device=img.device)

    def predict(self, x, profile=False, visualize=False, batch=None, augment=False):
        """
        Perform a forward pass through the model.

        Args:
            x (torch.Tensor): The input tensor.
            profile (bool, optional): If True, profile the computation time for each layer. Defaults to False.
            visualize (bool, optional): If True, save feature maps for visualization. Defaults to False.
            batch (dict, optional): Ground truth data for evaluation. Defaults to None.
            augment (bool, optional): If True, perform data augmentation during inference. Defaults to False.

        Returns:
            (torch.Tensor): Model's output tensor.
        """
        y, dt = [], []  # outputs
        for m in self.model[:-1]:  # except the head part
            if m.f != -1:  # if not from previous layer
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            if profile:
                self._profile_one_layer(m, x, dt)
            x = m(x)  # run
            y.append(x if m.i in self.save else None)  # save output
            if visualize:
                feature_visualization(x, m.type, m.i, save_dir=visualize)
        head = self.model[-1]
        x = head([y[j] for j in head.f], batch)  # head inference
        return x


if __name__ == '__main__':
    parser = argparse.ArgumentParser()  # åˆ›å»ºè§£æžå™¨
    # --cfg: æ¨¡åž‹é…ç½®æ–‡ä»¶
    parser.add_argument('--cfg', type=str, default='resnet10.yaml', help='model.yaml')
    # --device: é€‰ç”¨è®¾å¤‡
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    # --profile: ç”¨æˆ·é…ç½®æ–‡ä»¶
    parser.add_argument('--profile', action='store_true', help='profile model speed')
    # --test: æµ‹è¯•
    parser.add_argument('--test', action='store_true', help='test all yolo*.yaml')
    opt = parser.parse_args()  # å¢žåŠ åŽçš„å±žæ€§èµ‹å€¼ç»™args
    opt.cfg = check_yaml(opt.cfg)  # check YAML
    print_args(FILE.stem, opt)  # æ£€æµ‹YOLOçš„githubä»“åº“æ˜¯å¦æ›´æ–°,è‹¥å·²æ›´æ–°,ç»™å‡ºæç¤º
    device = select_device(opt.device)  # é€‰æ‹©è®¾å¤‡

    # Create model
    # æž„é€ æ¨¡åž‹
    model = Model(opt.cfg).to(device)
    # print(model)
    model.train()

    # Profile
    # ç”¨æˆ·è‡ªå®šä¹‰é…ç½®
    if opt.profile:
        img = torch.rand(8 if torch.cuda.is_available() else 1, 3, 640, 640).to(device)
        y = model(img, profile=True)

    # Test all models
    # æµ‹è¯•æ‰€æœ‰çš„æ¨¡åž‹
    if opt.test:
        for cfg in Path(ROOT / 'models').rglob('yolo*.yaml'):
            try:
                _ = Model(cfg)
            except Exception as e:
                print(f'Error in {cfg}: {e}')

    # Tensorboard (not working https://github.com/ultralytics/yolov5/issues/2898)
    # from torch.utils.tensorboard import SummaryWriter
    # tb_writer = SummaryWriter('.')
    # LOGGER.info("Run 'tensorboard --logdir=models' to view tensorboard at http://localhost:6006/")
    # tb_writer.add_graph(torch.jit.trace(model, img, strict=False), [])  # add model graph
